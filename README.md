# HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng Hadoop Cluster

[![Hadoop](https://img.shields.io/badge/Hadoop-3.4-orange)](https://hub.docker.com/r/apache/hadoop)
[![Spark](https://img.shields.io/badge/Spark-4.1.1-red)](https://hub.docker.com/r/apache/spark)
[![Docker](https://img.shields.io/badge/Docker-Compose-blue)](https://docs.docker.com/compose/)
[![Status](https://img.shields.io/badge/Status-Production%20Ready-green)]()

Há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u lá»›n vá»›i Hadoop vÃ  Spark, sáºµn sÃ ng sá»­ dá»¥ng vá»›i Docker. HÆ°á»›ng dáº«n nÃ y dÃ nh cho ngÆ°á»i dÃ¹ng cuá»‘i.

---

## ğŸ“– Giá»›i Thiá»‡u

ÄÃ¢y lÃ  há»‡ thá»‘ng xá»­ lÃ½ dá»¯ liá»‡u lá»›n (Big Data) cho phÃ©p báº¡n:
- âœ… LÆ°u trá»¯ file dá»¯ liá»‡u lá»›n.
- âœ… Xá»­ lÃ½ vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u báº±ng Python.
- âœ… MÃ´ phá»ng cháº¡y cÃ¡c tÃ¡c vá»¥ phÃ¢n tÃ¡n trÃªn nhiá»u mÃ¡y.
- âœ… Truy cáº­p dá»¯ liá»‡u qua giao diá»‡n web.

**KhÃ´ng cáº§n cÃ i Ä‘áº·t Hadoop hay Spark trá»±c tiáº¿p - táº¥t cáº£ cháº¡y trong Docker!**

---

## ğŸš€ Báº¯t Äáº§u Sá»­ Dá»¥ng

### BÆ°á»›c 1: Chuáº©n Bá»‹

Äáº£m báº£o báº¡n Ä‘Ã£ cÃ i Ä‘áº·t:
- âœ… Docker Desktop (pháº£i Ä‘ang cháº¡y)
- âœ… Ãt nháº¥t 4GB RAM trá»‘ng
- âœ… Khoáº£ng 5GB dung lÆ°á»£ng á»• cá»©ng

**Khuyáº¿n nghá»‹:** Táº¡o thÆ° má»¥c `data/` trong project Ä‘á»ƒ dá»… upload file:
```bash
mkdir data
```

### BÆ°á»›c 2: Khá»Ÿi Äá»™ng Há»‡ Thá»‘ng

Má»Ÿ Terminal/Command Prompt táº¡i thÆ° má»¥c dá»± Ã¡n vÃ  cháº¡y:

```bash
# Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng (máº¥t khoáº£ng 60 giÃ¢y)
docker compose up -d
```

**Äá»£i 60 giÃ¢y** Ä‘á»ƒ há»‡ thá»‘ng khá»Ÿi Ä‘á»™ng hoÃ n toÃ n, sau Ä‘Ã³ **tÃ¹y chá»n** khá»Ÿi táº¡o:

```bash
# VÃ o container hadoop-client
docker compose exec -it hadoop-client bash

# (TÃ¹y chá»n) Táº¡o cáº¥u trÃºc thÆ° má»¥c Ä‘á» xuáº¥t:
hdfs dfs -mkdir -p /data/raw /data/processed /data/backup

# Kiá»ƒm tra:
hdfs dfs -ls /
```

**ğŸ’¡ LÆ°u Ã½:** Trong thá»±c táº¿, HDFS tá»± Ä‘á»™ng táº¡o thÆ° má»¥c khi báº¡n upload file. BÆ°á»›c nÃ y chá»‰ Ä‘á»ƒ cÃ³ cáº¥u trÃºc rÃµ rÃ ng cho viá»‡c há»c táº­p.

**ğŸ“ ThÆ° má»¥c script:** `/spark-apps/` (tÆ°Æ¡ng á»©ng vá»›i `./spark-apps/` trÃªn mÃ¡y tÃ­nh)

### BÆ°á»›c 3: Kiá»ƒm Tra Há»‡ Thá»‘ng

Má»Ÿ trÃ¬nh duyá»‡t vÃ  truy cáº­p:
- **Quáº£n lÃ½ file:** http://localhost:9870
- **Quáº£n lÃ½ tÃ¡c vá»¥:** http://localhost:8088

Náº¿u tháº¥y giao diá»‡n web lÃ  há»‡ thá»‘ng Ä‘Ã£ hoáº¡t Ä‘á»™ng! ğŸ‰

---

## ğŸ’¡ CÃ¡ch Sá»­ Dá»¥ng Lá»‡nh

**Táº¥t cáº£ lá»‡nh dÆ°á»›i Ä‘Ã¢y cáº§n cháº¡y bÃªn trong terminal cá»§a container. CÃ¡ch lÃ m:**

```bash
# 1. VÃ o container hadoop-client (Ä‘á»ƒ quáº£n lÃ½ HDFS, file operations)
docker compose exec -it hadoop-client bash

# 2. Hoáº·c vÃ o container spark-master (Ä‘á»ƒ cháº¡y Python/PySpark)
docker compose exec -it spark-master bash

# 3. Cháº¡y cÃ¡c lá»‡nh trong terminal cá»§a container

# 4. ThoÃ¡t container khi xong
exit
```

**Ghi chÃº:** CÃ¡c pháº§n dÆ°á»›i Ä‘Ã¢y sáº½ chá»‰ rÃµ nÃªn dÃ¹ng container nÃ o.

### ğŸ” CÃ¡ch Nháº­n Biáº¿t Äang á» ÄÃ¢u

**BÃªn trong container (sau khi cháº¡y `docker compose exec -it ... bash`):**
```
root@hadoop-client:/# 
root@spark-master:/#
```
ğŸ‘‰ Prompt cÃ³ dáº¡ng `root@[tÃªn-container]:/#` - báº¡n cÃ³ thá»ƒ cháº¡y lá»‡nh HDFS, Python

**BÃªn ngoÃ i (terminal cá»§a mÃ¡y tÃ­nh):**
```
PS C:\Users\YourName\project>   # Windows
$ ~/project                      # Mac/Linux
```
---

## ğŸ’¼ CÃ¡c TÃ¡c Vá»¥ ThÆ°á»ng DÃ¹ng

### ğŸ“¤ Upload File LÃªn Há»‡ Thá»‘ng

**Container:** `hadoop-client`

**TÃ¬nh huá»‘ng:** Báº¡n cÃ³ file `data.csv` trÃªn mÃ¡y tÃ­nh vÃ  muá»‘n upload lÃªn há»‡ thá»‘ng.

#### ğŸ¯ DÃ¹ng ThÆ° Má»¥c Chia Sáº» `./data/`

Há»‡ thá»‘ng Ä‘Ã£ tá»± Ä‘á»™ng chia sáº» thÆ° má»¥c `data/` giá»¯a mÃ¡y tÃ­nh vÃ  container.

```bash
# BÆ°á»›c 1: Copy file vÃ o thÆ° má»¥c data/ trong project
# (báº¡n cÃ³ thá»ƒ kÃ©o tháº£ file vÃ o thÆ° má»¥c nÃ y)

# BÆ°á»›c 2: Kiá»ƒm tra file Ä‘Ã£ cÃ³
ls -lh /data-local/

# BÆ°á»›c 3: Upload file lÃªn HDFS
hdfs dfs -put /data-local/data.csv /data/raw/

# BÆ°á»›c 4: Kiá»ƒm tra file Ä‘Ã£ lÃªn HDFS
hdfs dfs -ls /data/raw/
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… KhÃ´ng cáº§n lá»‡nh `docker cp`
- âœ… File trong thÆ° má»¥c `data/` tá»± Ä‘á»™ng xuáº¥t hiá»‡n trong container
- âœ… Dá»… quáº£n lÃ½ nhiá»u file cÃ¹ng lÃºc
- âœ… CÃ³ thá»ƒ kÃ©o tháº£ file báº±ng chuá»™t trÃªn Windows

**LÆ°u Ã½:** Náº¿u thÆ° má»¥c `data/` chÆ°a tá»“n táº¡i, táº¡o báº±ng lá»‡nh `mkdir data`

---

### ğŸ“¥ Download File Tá»« Há»‡ Thá»‘ng

**Container:** `hadoop-client`

**TÃ¬nh huá»‘ng:** Báº¡n muá»‘n táº£i file káº¿t quáº£ vá» mÃ¡y tÃ­nh.

#### ğŸ¯ DÃ¹ng ThÆ° Má»¥c Chia Sáº» `./data/` (Khuyáº¿n Nghá»‹)

```bash
# BÆ°á»›c 1: Download file tá»« HDFS vá» thÆ° má»¥c chia sáº»
hdfs dfs -get /data/processed/result.csv /data-local/

# BÆ°á»›c 2: Kiá»ƒm tra file Ä‘Ã£ download
ls -lh /data-local/result.csv

# BÆ°á»›c 3: ThoÃ¡t container
exit

# File Ä‘Ã£ tá»± Ä‘á»™ng cÃ³ trong thÆ° má»¥c ./data/ trÃªn mÃ¡y tÃ­nh cá»§a báº¡n!
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… File tá»± Ä‘á»™ng xuáº¥t hiá»‡n trong thÆ° má»¥c `data/` trÃªn mÃ¡y
- âœ… KhÃ´ng cáº§n lá»‡nh `docker cp`
- âœ… Download nhiá»u file cÃ¹ng lÃºc: `hdfs dfs -get /data/processed/* /data-local/`

---

### ğŸ“‚ Xem Danh SÃ¡ch File

**Container:** `hadoop-client`

```bash
# Xem táº¥t cáº£ file trong há»‡ thá»‘ng
hdfs dfs -ls /

# Xem file trong thÆ° má»¥c cá»¥ thá»ƒ
hdfs dfs -ls /data/raw/

# Xem chi tiáº¿t dung lÆ°á»£ng
hdfs dfs -du -h /data/
```

### ğŸ“„ Äá»c Ná»™i Dung File

**Container:** `hadoop-client`

```bash
# Äá»c toÃ n bá»™ file
hdfs dfs -cat /data/raw/users.csv

# Äá»c 10 dÃ²ng Ä‘áº§u
hdfs dfs -cat /data/raw/users.csv | head -n 10

# Äá»c 10 dÃ²ng cuá»‘i
hdfs dfs -cat /data/raw/users.csv | tail -n 10
```

### ğŸ—‘ï¸ XÃ³a File

**Container:** `hadoop-client`

```bash
# XÃ³a má»™t file
hdfs dfs -rm /data/raw/old-file.csv

# XÃ³a thÆ° má»¥c vÃ  táº¥t cáº£ file bÃªn trong
hdfs dfs -rm -r /data/old-folder/
```

### ğŸ“ Táº¡o ThÆ° Má»¥c

**Container:** `hadoop-client`

```bash
# Táº¡o thÆ° má»¥c má»›i
hdfs dfs -mkdir -p /data/my-project/input
```

---

## ğŸ Xá»­ LÃ½ Dá»¯ Liá»‡u Vá»›i Python (PySpark)

### BÆ°á»›c 1: Táº¡o File Python

Táº¡o file `my_analysis.py` trong thÆ° má»¥c `spark-apps/` trÃªn mÃ¡y tÃ­nh cá»§a báº¡n (file sáº½ tá»± Ä‘á»™ng cÃ³ trong container vÃ¬ Ä‘Ã£ mount volume):

```python
from pyspark.sql import SparkSession

# Khá»Ÿi táº¡o Spark
spark = SparkSession.builder \
    .appName("My Data Analysis") \
    .master("local") \
    .getOrCreate()

# Äá»c file CSV tá»« HDFS
df = spark.read.csv(
    "hdfs://namenode:8020/data/raw/users.csv",
    header=True,
    inferSchema=True
)

# Xem dá»¯ liá»‡u
print("=== Dá»¯ liá»‡u Ä‘áº§u vÃ o ===")
df.show(10)

# Thá»±c hiá»‡n phÃ¢n tÃ­ch
print("=== Thá»‘ng kÃª ===")
df.describe().show()

# LÆ°u káº¿t quáº£
df.write.mode("overwrite").csv(
    "hdfs://namenode:8020/data/processed/result",
    header=True
)

print("HoÃ n thÃ nh!")
spark.stop()
```

### BÆ°á»›c 2: Cháº¡y Script

**Container:** `spark-master`

```bash
# Cháº¡y script
python3 /spark-apps/my_analysis.py
```

### BÆ°á»›c 3: Xem Káº¿t Quáº£

**Container:** `hadoop-client`

```bash
hdfs dfs -ls /data/processed/result/
hdfs dfs -cat /data/processed/result/*.csv | head -n 20
```

---

## ğŸ¯ CÃ¡c VÃ­ Dá»¥ Thá»±c Táº¿

### VÃ­ Dá»¥ 1: Äáº¿m Sá»‘ DÃ²ng Trong File

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Count Rows").master("local").getOrCreate()
df = spark.read.csv("hdfs://namenode:8020/data/raw/users.csv", header=True)

total_rows = df.count()
print(f"Tá»•ng sá»‘ dÃ²ng: {total_rows}")

spark.stop()
```

### VÃ­ Dá»¥ 2: Lá»c Dá»¯ Liá»‡u

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Filter Data").master("local").getOrCreate()
df = spark.read.csv("hdfs://namenode:8020/data/raw/users.csv", header=True)

# Lá»c user cÃ³ age > 25
filtered_df = df.filter(df.age > 25)
filtered_df.show()

# LÆ°u káº¿t quáº£
filtered_df.write.mode("overwrite").csv(
    "hdfs://namenode:8020/data/processed/filtered_users",
    header=True
)

spark.stop()
```

### VÃ­ Dá»¥ 3: Gá»™p Nhiá»u File

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Merge Files").master("local").getOrCreate()

# Äá»c táº¥t cáº£ file CSV trong thÆ° má»¥c
df = spark.read.csv("hdfs://namenode:8020/data/raw/*.csv", header=True)

print(f"Tá»•ng sá»‘ dÃ²ng sau khi gá»™p: {df.count()}")

# LÆ°u thÃ nh 1 file duy nháº¥t
df.coalesce(1).write.mode("overwrite").csv(
    "hdfs://namenode:8020/data/processed/merged",
    header=True
)

spark.stop()
```

---

## ğŸ”§ Quáº£n LÃ½ Há»‡ Thá»‘ng

### Táº¯t Há»‡ Thá»‘ng (Giá»¯ Dá»¯ Liá»‡u)

```bash
# Táº¡m dá»«ng - dá»¯ liá»‡u váº«n cÃ²n
docker compose stop
```

### Khá»Ÿi Äá»™ng Láº¡i

```bash
# Cháº¡y láº¡i há»‡ thá»‘ng - dá»¯ liá»‡u váº«n nguyÃªn
docker compose start
```

### Khá»Ÿi Äá»™ng Láº¡i HoÃ n ToÃ n

```bash
# Khá»Ÿi Ä‘á»™ng láº¡i táº¥t cáº£
docker compose restart
```

### XÃ³a VÃ  LÃ m Má»›i

```bash
# XÃ³a containers nhÆ°ng giá»¯ dá»¯ liá»‡u
docker compose down

# XÃ³a hoÃ n toÃ n (cáº£ dá»¯ liá»‡u) - THáº¬N TRá»ŒNG!
docker compose down -v
```

âš ï¸ **LÆ°u Ã½:** Náº¿u cháº¡y `docker compose down -v`, báº¡n sáº½ máº¥t táº¥t cáº£ dá»¯ liá»‡u vÃ  pháº£i cháº¡y láº¡i script khá»Ÿi táº¡o.

---

## ğŸ“Š Xem Tráº¡ng ThÃ¡i Há»‡ Thá»‘ng

### Xem Dung LÆ°á»£ng

**Container:** `hadoop-client`

```bash
# Xem dung lÆ°á»£ng Ä‘Ã£ dÃ¹ng
hdfs dfs -df -h /

# Xem dung lÆ°á»£ng tá»«ng thÆ° má»¥c
hdfs dfs -du -h /data/
```

### Kiá»ƒm Tra CÃ¡c Node

**Container:** `hadoop-client`

```bash
# Xem cÃ¡c node Ä‘ang cháº¡y
yarn node -list

# Xem tráº¡ng thÃ¡i HDFS
hdfs dfsadmin -report
```

### Xem CÃ¡c TÃ¡c Vá»¥ Äang Cháº¡y

**Container:** `hadoop-client`

```bash
# Liá»‡t kÃª tÃ¡c vá»¥ Ä‘ang cháº¡y
yarn application -list

# Hoáº·c truy cáº­p: http://localhost:8088
```

---

## â“ Xá»­ LÃ½ Sá»± Cá»‘

### Há»‡ Thá»‘ng KhÃ´ng Khá»Ÿi Äá»™ng

```bash
# Xem log Ä‘á»ƒ biáº¿t lá»—i
docker compose logs namenode
docker compose logs datanode1
```

### File Upload Bá»‹ Lá»—i

**Container:** `hadoop-client`

```bash
# Kiá»ƒm tra HDFS Ä‘ang cháº¡y
hdfs dfs -ls /

# Náº¿u bÃ¡o "safe mode", cháº¡y:
hdfs dfsadmin -safemode leave
```

### Python Script BÃ¡o Lá»—i

**Container:** `spark-master`

```bash
# Kiá»ƒm tra PySpark
python3 -c "from pyspark.sql import SparkSession; print('OK')"

# Xem log chi tiáº¿t khi cháº¡y script
python3 /spark-apps/your-script.py 2>&1 | more
```

### Giao Diá»‡n Web KhÃ´ng Má»Ÿ

```bash
# Kiá»ƒm tra containers Ä‘ang cháº¡y
docker compose ps

# Khá»Ÿi Ä‘á»™ng láº¡i náº¿u cáº§n
docker compose restart namenode resourcemanager
```

---

## ğŸ“ Máº¹o Sá»­ Dá»¥ng

### 1. LÃ m Viá»‡c Vá»›i File Lá»›n

**Container:** `hadoop-client`

Khi upload file lá»›n (>1GB):
```bash
hdfs dfs -put -f /path/to/large-file.csv /data/raw/
```

### 2. Kiá»ƒm Tra Nhanh Python Script

**Container:** `spark-master`

TrÆ°á»›c khi cháº¡y script phá»©c táº¡p, test nhanh:
```bash
python3 -c "
from pyspark.sql import SparkSession
spark = SparkSession.builder.master('local').getOrCreate()
print('Spark version:', spark.version)
spark.stop()
"
```

### 3. Backup Dá»¯ Liá»‡u

**Container:** `hadoop-client`

```bash
# Download toÃ n bá»™ thÆ° má»¥c vá» container
hdfs dfs -get /data/processed /tmp/backup/

# ThoÃ¡t vÃ  copy vá» mÃ¡y
exit
docker cp hadoop-client:/tmp/backup ./backup/
```

### 4. Xem Log Real-time

```bash
# Theo dÃµi log trá»±c tiáº¿p
docker compose logs -f namenode
```

---

## ğŸ“ Trá»£ GiÃºp ThÃªm

### Giao Diá»‡n Web

| Trang | Äá»‹a Chá»‰ | Má»¥c ÄÃ­ch |
|-------|---------|----------|
| Quáº£n lÃ½ file | http://localhost:9870 | Xem file, dung lÆ°á»£ng, tráº¡ng thÃ¡i |
| Quáº£n lÃ½ tÃ¡c vá»¥ | http://localhost:8088 | Xem job Ä‘ang cháº¡y, lá»‹ch sá»­ |

### CÃ¡c Lá»‡nh Há»¯u Ãch

```bash
# Copy file giá»¯a containers
docker cp local-file.txt hadoop-client:/tmp/

# Cháº¡y lá»‡nh shell trong container
docker compose exec -it hadoop-client bash

# Xem IP cá»§a containers (khÃ´ng cáº§n -it cho lá»‡nh ngáº¯n)
docker compose exec hadoop-client hostname -i
```

---

## âœ… Checklist Sá»­ Dá»¥ng HÃ ng NgÃ y

- [ ] Kiá»ƒm tra Docker Desktop Ä‘ang cháº¡y
- [ ] Cháº¡y `docker compose ps` Ä‘á»ƒ xem services Ä‘ang up
- [ ] Truy cáº­p http://localhost:9870 Ä‘á»ƒ xÃ¡c nháº­n HDFS hoáº¡t Ä‘á»™ng
- [ ] Upload file dá»¯ liá»‡u cáº§n xá»­ lÃ½
- [ ] Cháº¡y Python script phÃ¢n tÃ­ch
- [ ] Xem káº¿t quáº£ trÃªn HDFS hoáº·c download vá» mÃ¡y
- [ ] Táº¯t há»‡ thá»‘ng vá»›i `docker compose stop` khi khÃ´ng dÃ¹ng

---

## ğŸ¯ Workflow TiÃªu Biá»ƒu

```
1. Chuáº©n bá»‹ dá»¯ liá»‡u â†’ data.csv
2. Upload â†’ hdfs dfs -put data.csv /data/raw/
3. Viáº¿t script Python â†’ my_analysis.py  
4. Cháº¡y script â†’ python3 my_analysis.py
5. Xem káº¿t quáº£ â†’ hdfs dfs -cat /data/processed/result/*.csv
6. Download vá» mÃ¡y â†’ hdfs dfs -get /data/processed/result ./
```

---

**ChÃºc báº¡n sá»­ dá»¥ng hiá»‡u quáº£! ğŸš€**

Náº¿u gáº·p váº¥n Ä‘á» khÃ´ng giáº£i quyáº¿t Ä‘Æ°á»£c, hÃ£y kiá»ƒm tra log vá»›i `docker compose logs [tÃªn-service]`
